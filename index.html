<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gowthami Somepalli</title>
    <link rel="icon" href="favicons/favicon.ico">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <nav>
            <a href="index.html" class="active">home</a>
            <a href="blog.html">blog</a>
            <a href="notes.html">notes</a>
            <a href="files/Gowthami_CV.pdf">cv</a>
        </nav>

        <header>
            <div class="header-content">
                <div class="intro">
                    <h1>Gowthami Somepalli</h1>
                    <p class="tagline"> Research Scientist @ Adobe · Post-training · Diffusion Models<span class="cursor">|</span></p>

                    <div class="links">
                        <a href="https://github.com/somepago">github</a>
                        <a href="https://scholar.google.com/citations?user=T2ezBDsAAAAJ">scholar</a>
                        <a href="https://twitter.com/gowthami_s">twitter</a>
                        <a href="https://www.linkedin.com/in/gowthamis/">linkedin</a>
                    </div>
                </div>
                <div class="portrait-container">
                    <img src="files/g_dp_crop.jpg" alt="Gowthami Somepalli" class="portrait">
                </div>
            </div>
        </header>

        <main>
            <section class="about">
                <p>
                    Multimodal AI researcher obsessed with how machines perceive, remember, and generate the world. Based in Mountain View, CA. <em>(Friends call me the "Evals Shill" for a reason.)</em>
                    Currently post-training Adobe's image gen models to push creative boundaries.
                </p>
                <p>
                    PhD from UMD focused on diffusion model memorization. Built evals that actually test video understanding – like <a href="https://ruchitrawal.github.io/cinepile/">CinePile</a> (long-video QA benchmark, Best Paper at CVPR 2024 SynCV) and <a href="https://ruchitrawal.github.io/argus/">ARGUS</a> (hallucination/omission eval for dense captions).
                </p>
                <p>
                    Before academia: did SGD in industry for a while in India, IIT Madras alum, founded a Fashion AI startup <em>that was way too early to the party</em>. 
                </p>
                <p class="contact">
                    Open to collabs on generative modeling (evals + post-training). Hit me up: gowthami [dot] somepalli [at] gmail.com
                </p>
            </section>

            <section class="featured-posts">
                <h2>// featured writing</h2>
                <div class="post-grid">
                    <a href="blog.html" class="post-card">
                        <span class="post-tag">BLOG POST: coming soon</span>
                        <h3>Memorization in Diffusion Models</h3>
                        <p>Three papers later, here's what I've learned about copying in generative models.</p>
                    </a>
                    <a href="blog.html" class="post-card">
                        <span class="post-tag">BLOG POST: coming soon</span>
                        <h3>Distillation landscape in image generation</h3>
                        <p>Some thoughts on the distillation landscape in image generation.</p>
                    </a>
                </div>
            </section>

            <section class="papers">
                <h2>// papers</h2>

                <article class="paper">
                    <span class="year">2025</span>
                    <div class="paper-content">
                        <a href="https://ruchitrawal.github.io/argus/">ARGUS: A Hallucination and Omission Evaluation Benchmark for Dense Video Captioning</a>
                        <span class="paper-meta"><span class="venue">ECCV</span><span class="paper-links"><a href="https://arxiv.org/abs/2506.07371">paper</a> · <a href="https://huggingface.co/datasets/tomg-group-umd/argus">dataset</a> · <a href="https://github.com/JARVVVIS/argus">code</a></span></span>
                    </div>
                </article>
                <article class="paper">
                    <span class="year">2024</span>
                    <div class="paper-content">
                        <a href="files/CALVIN_Neurips.pdf">Calvin: Improved Contextual Video Captioning via Instruction Tuning</a>
                        <span class="paper-meta"><span class="venue">NeurIPS</span><span class="paper-links"><a href="files/CALVIN_Neurips.pdf">paper</a></span></span>
                    </div>
                </article>

                <article class="paper">
                    <span class="year">2024</span>
                    <div class="paper-content">
                        <a href="https://ruchitrawal.github.io/cinepile/">CinePile: A Long Video Question Answering Dataset and Benchmark</a>
                        <span class="paper-meta"><span class="venue">CVPR Workshop · Best Paper</span><span class="paper-links"><a href="https://arxiv.org/abs/2405.08813">paper</a> · <a href="https://huggingface.co/datasets/tomg-group-umd/cinepile">dataset</a></span></span>
                    </div>
                </article>

                <article class="paper">
                    <span class="year">2024</span>
                    <div class="paper-content">
                        <a href="csd.html">Measuring Style Similarity in Diffusion Models</a>
                        <span class="paper-meta"><span class="venue">ECCV</span><span class="paper-links"><a href="https://arxiv.org/abs/2404.01292">paper</a> · <a href="csd.html">project</a> · <a href="https://github.com/learn2phoenix/CSD">code</a></span></span>
                    </div>
                </article>

                <article class="paper">
                    <span class="year">2023</span>
                    <div class="paper-content">
                        <a href="https://arxiv.org/abs/2305.20086">Understanding and Mitigating Copying in Diffusion Models</a>
                        <span class="paper-meta"><span class="venue">NeurIPS</span><span class="paper-links"><a href="https://arxiv.org/abs/2305.20086">paper</a> · <a href="https://github.com/somepago/DCR">code</a> · <a href="https://www.youtube.com/watch?v=lubcx0p_0ic">talk</a></span></span>
                    </div>
                </article>

                <article class="paper">
                    <span class="year">2023</span>
                    <div class="paper-content">
                        <a href="diffrep.html">Diffusion Art or Digital Forgery? Investigating Data Replication</a>
                        <span class="paper-meta"><span class="venue">CVPR</span><span class="paper-links"><a href="https://arxiv.org/abs/2212.03860">paper</a> · <a href="diffrep.html">project</a> · <a href="https://github.com/somepago/DCR">code</a> · <a href="https://techcrunch.com/2022/12/13/image-generating-ai-can-copy-and-paste-from-training-data-raising-ip-concerns/">press</a></span></span>
                    </div>
                </article>

                <article class="paper">
                    <span class="year">2022</span>
                    <div class="paper-content">
                        <a href="dbviz.html">Can Neural Nets Learn the Same Model Twice?</a>
                        <span class="paper-meta"><span class="venue">CVPR · Oral</span><span class="paper-links"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Somepalli_Can_Neural_Nets_Learn_the_Same_Model_Twice_Investigating_Reproducibility_CVPR_2022_paper.pdf">paper</a> · <a href="dbviz.html">project</a> · <a href="https://github.com/somepago/dbViz">code</a> · <a href="https://www.youtube.com/watch?v=r50DRLLD-po">video</a></span></span>
                    </div>
                </article>

                <article class="paper">
                    <span class="year">2022</span>
                    <div class="paper-content">
                        <a href="saint.html">SAINT: Neural Networks for Tabular Data</a>
                        <span class="paper-meta"><span class="venue">NeurIPS TRLW</span><span class="paper-links"><a href="https://openreview.net/pdf?id=FiyUTAy4sB8">paper</a> · <a href="saint.html">project</a> · <a href="https://github.com/somepago/saint">code</a></span></span>
                    </div>
                </article>

                <article class="paper">
                    <span class="year">2021</span>
                    <div class="paper-content">
                        <a href="https://kampta.github.io/patch-game/">PatchGame: Learning to Signal Mid-level Patches</a>
                        <span class="paper-meta"><span class="venue">NeurIPS</span><span class="paper-links"><a href="https://proceedings.neurips.cc/paper/2021/hash/dac32839a9f0baae954b41abee610cc0-Abstract.html">paper</a> · <a href="https://github.com/kampta/PatchGame">code</a> · <a href="https://papertalk.org/papertalks/37423">video</a></span></span>
                    </div>
                </article>

                <p class="more-papers"><a href="https://scholar.google.com/citations?user=T2ezBDsAAAAJ">all papers →</a></p>
            </section>

            <section class="news">
                <h2>// news</h2>
                <ul>
                    <li><span class="date">Jul '24</span> 2 papers at NeurIPS'24</li>
                    <li><span class="date">Jul '24</span> CSD accepted to ECCV'24</li>
                    <li><span class="date">Jun '24</span> Best paper award at Synth4CV, CVPR</li>
                    <li><span class="date">Mar '24</span> Ann-Wylie Fellowship</li>
                    <li><span class="date">Dec '23</span> Talk at NeurIPS Diffusion Workshop</li>
                    <li><span class="date">Dec '22</span> Work covered in <a href="https://techcrunch.com/2022/12/13/image-generating-ai-can-copy-and-paste-from-training-data-raising-ip-concerns/">TechCrunch</a></li>
                </ul>
            </section>
        </main>

        <footer>
            <p>© 2025 · built with claude ❤️</p>
        </footer>
    </div>
</body>
</html>
